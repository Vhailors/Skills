---
name: 50klph-data-pipeline
description: Use this skill when working with the 50kLinesPerHour project's data pipeline that connects AgenticSpecKit script execution to the dashboard frontend. This skill should be invoked when debugging why generated files aren't showing in the dashboard, why logs aren't updating in real-time, when adding new real-time data streams, or when connecting script output to backend services. It provides systematic workflows for diagnosing pipeline gaps, filtering logs for user display, implementing debouncing, and ensuring state-driven dashboard updates.
---

# 50kLinesPerHour Data Pipeline Skill

## Overview

The 50kLinesPerHour dashboard presents data from projects generated by the AgenticSpecKit script. Data flows through a three-layer pipeline: **Script (subprocess) → Backend (agents + database) → Frontend (SSE streams + REST API)**. This skill provides systematic workflows, debugging utilities, and implementation patterns for maintaining and extending this data pipeline.

## When to Use This Skill

Invoke this skill when:
- **Debugging data gaps**: "Generated files not showing in dashboard"
- **Adding new streams**: Implementing new real-time data types (logs/insights/knowledge)
- **Log filtering**: Hiding implementation details while keeping user-friendly information
- **Performance issues**: Dashboard updates too frequent or too slow
- **Connection issues**: Frontend not receiving backend updates
- **State synchronization**: Ensuring dashboard reflects actual job state

## Core Concepts

### Three-Layer Architecture

```
Script (AgenticSpecKit subprocess)
  ↓ stdout/stderr + file system
Backend (Executor, Scanner, Observer agents)
  ↓ SQLite DB + SSE streams
Frontend (Next.js dashboard components)
```

### Data Hierarchy

```
User → Project → Job → Logs/Artifacts
```

All logs and artifacts are connected through the `job_id` foreign key, which links to `project_id` and `user_id`.

### Critical Agents

1. **Executor** (`executor_v2.py`) - Spawns subprocesses, captures logs
2. **Artifact Scanner** (`artifact_scanner.py`) - Tracks generated files
3. **Observer** (`observer.py`) - Generates insights
4. **Curator** (`curator.py`) - Manages knowledge facts

## Workflow: Debugging Data Gaps

When data doesn't appear in the dashboard, follow this systematic approach:

### Step 1: Run the Pipeline Debugger

Use the included `debug_pipeline.py` script to identify where data gets stuck:

```bash
python scripts/debug_pipeline.py --job-id {job_id}
```

This checks four stages:
1. ✅/❌ Script generated files (file system)
2. ✅/❌ Backend captured logs (database)
3. ✅/❌ Artifact scanner found files (database)
4. ✅/❌ SSE streams active (recent messages)

### Step 2: Focus on the Failing Stage

**If Stage 1 fails (no files):**
- Script didn't run or failed
- Check `job.error_message` in database
- Look for subprocess errors in backend logs

**If Stage 2 fails (no logs):**
- Executor not capturing subprocess output
- Check executor agent is running
- Verify `log_service.create_log()` is being called

**If Stage 3 fails (files exist but not in DB):**
- Path mismatch in artifact scanner
- Verify `artifact_service.py` path calculation (4 levels up)
- Check scanner is running (should log every 5 seconds)

**If Stage 4 fails (no SSE activity):**
- Frontend not subscribed to streams
- Check `stream_service.broadcast_message()` is called
- Verify CORS settings allow frontend origin

### Step 3: Consult References

Load the relevant reference documentation:

```python
# For understanding the full architecture
Read references/architecture.md

# For specific troubleshooting steps
Read references/troubleshooting.md
```

The troubleshooting reference provides detailed fixes for common issues.

## Workflow: Filtering Logs for Users

Script logs contain implementation details that confuse users. Always filter before display.

### Step 1: Parse Raw Logs

Use the `log_parser.py` script:

```python
from scripts.log_parser import parse_log_line, should_show_to_user

# In executor_v2.py when processing subprocess output
raw_line = await process.stdout.readline()
result = parse_log_line(raw_line.decode())

if should_show_to_user(result):
    # Send to database and dashboard
    await log_service.create_log(
        db,
        job_id,
        result['message'],  # Sanitized message (paths removed)
        result['level']      # info/warning/error/debug
    )
```

### Step 2: Understand Filtering Rules

The parser automatically:
- ✅ **Hides**: Debug logs, SQLAlchemy internals, tracebacks, cache details
- ✅ **Shows**: Errors (always), progress indicators, user-facing info
- ✅ **Sanitizes**: Removes absolute paths (`/mnt/c/Users/.../Documents/...`)
- ✅ **Detects phases**: Identifies specify/plan/implement phases

### Step 3: Handle Phase Detection

Extract detected phases to update job state:

```python
result = parse_log_line(raw_line)
phase = result.get('phase')  # 'specify', 'plan', 'implement', etc.

if phase and phase != current_job.current_phase:
    await job_service.update_job(
        db,
        job_id,
        JobUpdate(current_phase=phase)
    )
```

## Workflow: Implementing Debouncing

Prevent overwhelming the frontend with too many updates.

### Step 1: Use the Dashboard State Manager

```python
from scripts.dashboard_state import DashboardStateManager, JobState

manager = DashboardStateManager()

job = JobState(
    job_id=job.id,
    status=job.status,
    current_phase=job.current_phase,
    lines_written=job.lines_written
)
```

### Step 2: Calculate Updates Based on Event Type

```python
# On log event (debounced to 500ms)
updates = manager.calculate_updates(job, 'log')
if updates.logs_panel:
    await stream_service.broadcast_message('logs', message, 'system')

# On phase change (forced, ignores debouncing)
updates = manager.calculate_updates(job, 'phase_change', force=True)
if updates.phase_indicator:
    # Update phase indicator component
    await stream_service.broadcast_message('insights', f"Phase: {job.current_phase}", 'system')

# On progress (debounced to 1s)
updates = manager.calculate_updates(job, 'progress')
if updates.job_progress:
    # Update progress metrics
    await stream_service.broadcast_message('insights', f"{job.lines_written} lines", 'system')
```

### Step 3: Implement 15-Minute Insights

For observer agent insights:

```python
# In observer.py
if manager.should_generate_insight(job):
    insight = f"Job {job_id} in {job.current_phase} phase"
    await stream_service.broadcast_message('insights', insight, 'observer')
```

## Workflow: Adding a New Data Stream

To add a new real-time data type (e.g., "metrics" stream):

### Step 1: Update Database Schema

```sql
-- Add to stream_messages check constraint
ALTER TABLE stream_messages DROP CONSTRAINT stream_type_check;
ALTER TABLE stream_messages ADD CONSTRAINT stream_type_check
  CHECK(stream_type IN ('logs', 'insights', 'knowledge', 'metrics'));
```

### Step 2: Update Stream Service

```python
# In src/services/stream_service.py
class StreamService:
    def __init__(self):
        self.message_queues: dict[StreamType, asyncio.Queue] = {
            'logs': asyncio.Queue(),
            'insights': asyncio.Queue(),
            'knowledge': asyncio.Queue(),
            'metrics': asyncio.Queue(),  # Add new stream
        }
```

### Step 3: Create SSE Endpoint

```python
# In src/api/routes/streams.py
@router.get("/streams/metrics")
async def metrics_stream():
    return StreamingResponse(
        stream_service.create_sse_generator('metrics'),
        media_type="text/event-stream"
    )
```

### Step 4: Broadcast Messages

```python
# From any agent or service
await stream_service.broadcast_message(
    'metrics',
    f"Lines/hour: {lines_per_hour}",
    'system'
)
```

### Step 5: Subscribe in Frontend

```typescript
// In frontend component
const { messages } = useEventSource('/streams/metrics');
```

## Common Patterns

### Pattern: Capturing Subprocess Output

```python
# In executor_v2.py
process = await asyncio.create_subprocess_exec(
    'claude', 'code', '--execute', command,
    stdout=asyncio.subprocess.PIPE,
    stderr=asyncio.subprocess.STDOUT,
    cwd=str(project_path)
)

while True:
    line = await process.stdout.readline()
    if not line:
        break

    # Parse and filter
    result = parse_log_line(line.decode())

    if should_show_to_user(result):
        await log_service.create_log(db, job_id, result['message'], result['level'])

    # Detect phase changes
    if result.get('phase'):
        await update_job_phase(db, job_id, result['phase'])
```

### Pattern: Scanning for Artifacts

```python
# In artifact_scanner.py
for project_dir in projects_root.iterdir():
    for file_path in os.walk(project_dir):
        # Skip excluded directories
        if any(skip in str(file_path) for skip in ['node_modules', '.git']):
            continue

        # Check mtime
        mtime = os.stat(file_path).st_mtime
        if file_path in last_scan_time and mtime <= last_scan_time[file_path]:
            continue  # Unchanged

        # Upsert to database
        await artifact_service.upsert_artifact(db, file_path, ...)
        last_scan_time[file_path] = mtime
```

### Pattern: State-Driven Updates

```python
# Update different components based on job state
if job.status == 'completed':
    # Scan all artifacts
    await artifact_service.scan_artifacts()
    # Update metrics
    await metrics_service.calculate_final_metrics(job_id)
    # Set output ready
    await job_service.update_job(db, job_id, JobUpdate(output_ready=True))

elif job.current_phase == 'implement':
    # High-frequency progress updates
    updates = manager.calculate_updates(job, 'progress')
    if updates.job_progress:
        await stream_service.broadcast_message('insights', f"{job.lines_written} lines", 'system')
```

## Troubleshooting Quick Reference

| Symptom | Most Likely Cause | Quick Fix |
|---------|------------------|-----------|
| Logs not showing | Executor not running | Check `main.py` startup includes `executor_v2_loop()` |
| Files not in artifacts | Path mismatch | Verify `artifact_service.py` line 32 (4 levels up) |
| Dashboard not updating | Client not subscribed | Check frontend uses `useEventSource()` hook |
| Phase stuck | Pattern not matching | Update regex in `executor_v2.py` lines 46-54 |
| Too many updates | No debouncing | Use `DashboardStateManager` from `scripts/` |

For detailed troubleshooting, consult `references/troubleshooting.md`.

## Resources

### scripts/

- **log_parser.py** - Parse and filter speckit subprocess logs
- **dashboard_state.py** - Calculate dashboard updates with debouncing
- **debug_pipeline.py** - Diagnose where data gets stuck

All scripts can be executed standalone or imported as modules.

### references/

- **architecture.md** - Complete system architecture and data flow diagrams
- **troubleshooting.md** - Detailed fixes for common pipeline issues

Load references as needed using the Read tool to get detailed information about specific areas.
