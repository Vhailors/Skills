# 50kLinesPerHour Data Pipeline Architecture

## Overview

The 50kLinesPerHour dashboard displays data from projects generated by the AgenticSpecKit script. The data flows through a three-layer pipeline:

```
┌─────────────────────────────────────────────────────────────────┐
│                    AgenticSpecKit Script                        │
│  (Subprocess running speckit specify/plan/implement commands)   │
└────────────────────────┬────────────────────────────────────────┘
                         │ stdout/stderr
                         │ File system (generated_projects/)
                         ↓
┌─────────────────────────────────────────────────────────────────┐
│                    Backend (FastAPI + Agents)                    │
│  ┌─────────────┐  ┌──────────────┐  ┌────────────────────┐     │
│  │  Executor   │  │   Artifact   │  │   Observer/Curator │     │
│  │   Agent     │  │    Scanner   │  │       Agents       │     │
│  └──────┬──────┘  └──────┬───────┘  └─────────┬──────────┘     │
│         │                │                     │                 │
│         └────────────────┼─────────────────────┘                 │
│                          ↓                                       │
│                   SQLite Database                                │
│         ┌────────────────┼────────────────────┐                 │
│         ↓                ↓                    ↓                  │
│    log_entries      artifacts        stream_messages            │
│    (job-specific)   (files)          (broadcasts)               │
└────────────────────────┬────────────────────────────────────────┘
                         │ SSE Streams / REST API
                         ↓
┌─────────────────────────────────────────────────────────────────┐
│                  Frontend Dashboard (Next.js)                    │
│  ┌──────────┐  ┌──────────┐  ┌─────────┐  ┌────────────┐       │
│  │  Phase   │  │  Kanban  │  │  Logs   │  │ Artifacts  │       │
│  │Indicator │  │  Board   │  │  Panel  │  │   Viewer   │       │
│  └──────────┘  └──────────┘  └─────────┘  └────────────┘       │
└─────────────────────────────────────────────────────────────────┘
```

## Layer 1: Script (AgenticSpecKit)

**What it does:**
- Runs as a subprocess spawned by executor_v2.py
- Executes workflow phases: specify → plan → implement → test → review
- Generates files in `generated_projects/{project_name}/`
- Outputs logs to stdout/stderr

**Key Files:**
- Generated project files (code, documentation)
- `spec.md`, `plan.md`, `tasks.md` (workflow artifacts)

**Capture Mechanism:**
- Executor captures subprocess stdout/stderr using `asyncio.create_subprocess_exec()`
- Logs are streamed line-by-line in real-time

## Layer 2: Backend Agents

### Executor Agent (executor_v2.py)

**Responsibilities:**
1. Poll for pending jobs
2. Spawn speckit subprocess for each job
3. Capture and parse subprocess output
4. Detect phase changes using regex patterns
5. Store logs in `log_entries` table
6. Update job status and metrics

**Log Flow:**
```python
subprocess.stdout →
  readline() →
    parse_log_line() →
      log_service.create_log() →
        SQLite (log_entries)
```

### Artifact Scanner Agent (artifact_scanner.py)

**Responsibilities:**
1. Scan `generated_projects/` directory every 5 seconds
2. Track file modifications using mtime
3. Generate previews (first 200 lines)
4. Store in `artifacts` table

**File Flow:**
```python
os.walk(generated_projects) →
  check mtime →
    artifact_service.upsert_artifact() →
      SQLite (artifacts)
```

### Observer/Curator Agents

**Responsibilities:**
- Observer: Generate insights about job progress
- Curator: Manage knowledge facts

**Broadcast Flow:**
```python
agent logic →
  stream_service.broadcast_message() →
    SQLite (stream_messages) +
    asyncio.Queue (for SSE)
```

## Layer 3: Frontend Dashboard

**Data Sources:**
1. **REST API** - Initial page load, user actions
2. **SSE Streams** - Real-time updates

**SSE Streams:**
- `/streams/logs` - System logs
- `/streams/insights` - Observer insights
- `/streams/knowledge` - Curator knowledge
- `/jobs/{job_id}/logs/stream` - Job-specific logs

**Components:**
- Phase Indicator - Shows current job phase
- Kanban Board - Job queue and status
- Logs Panel - Real-time log display
- Artifacts Viewer - Browse generated files
- Job Progress Panel - Metrics and progress

## Database Schema

### User → Project → Job Hierarchy

```
users
├─ id (PK)
├─ email
├─ role (admin/user)
└─ line_credits

projects
├─ id (PK)
├─ name
└─ user_id (FK → users.id)

jobs
├─ id (PK)
├─ project_id (FK → projects.id)
├─ user_id (FK → users.id)
├─ specification
├─ status (pending/in_progress/completed/failed)
├─ current_phase (idle/specify/plan/implement/test/review)
└─ lines_written

log_entries
├─ id (PK)
├─ job_id (FK → jobs.id)
├─ message
├─ level (info/warning/error/debug)
├─ source (system/agent/user)
└─ timestamp

artifacts
├─ id (PK)
├─ file_path
├─ file_name
├─ artifact_type (documentation/code)
├─ preview_content (first 200 lines)
└─ created_at

stream_messages
├─ id (PK)
├─ stream_type (logs/insights/knowledge)
├─ message_text
├─ source (system/observer/curator)
└─ created_at
```

## Common Data Flow Patterns

### Pattern 1: Job Execution

1. User creates job via `/projects/{project}/jobs` POST
2. Job status = "pending"
3. Executor polls for pending jobs
4. Executor spawns speckit subprocess
5. Job status → "in_progress"
6. Subprocess outputs logs
7. Executor captures and stores logs in `log_entries`
8. Phase changes detected → job.current_phase updated
9. Frontend polls `/jobs/{job_id}` or subscribes to `/jobs/{job_id}/logs/stream`
10. Dashboard updates in real-time

### Pattern 2: Artifact Discovery

1. Script generates files in `generated_projects/EmailScrapper/`
2. Artifact scanner runs every 5 seconds
3. Scanner walks directory tree
4. For each file, check mtime against last scan
5. If modified, upsert to `artifacts` table
6. Frontend fetches `/artifacts?type=documentation` or `/artifacts?type=code`
7. Artifacts viewer displays files

### Pattern 3: Insight Broadcasting

1. Observer agent runs every 30 seconds
2. Checks current job state
3. Generates insight text
4. Calls `stream_service.broadcast_message('insights', text, 'observer')`
5. Message stored in `stream_messages`
6. Message pushed to SSE queue
7. Frontend subscribed to `/streams/insights` receives update
8. Insights panel displays message

## Critical Paths

These are the most common places where data "gets stuck":

1. **Script → Executor**: Subprocess may fail silently, executor not capturing output
2. **Executor → Database**: Logs parsed but not saved (transaction rollback)
3. **Database → SSE**: Messages stored but queue not notified
4. **File System → Artifact Scanner**: Path mismatch, scanner looking in wrong directory
5. **SSE → Frontend**: Client disconnected, not subscribed to stream
